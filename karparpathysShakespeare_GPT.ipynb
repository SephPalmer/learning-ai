{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNQVchjwe5nYejxJR06iDRD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgo6Jp-k5LL9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d9d3d7c"
      },
      "source": [
        "# Task\n",
        "Implement and train a GPT language model on the TinyShakespeare dataset using PyTorch and `tiktoken`, including data preparation, defining the Transformer architecture with Multi-Head Attention, and generating sample text after training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "992df8e4"
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "### Subtask:\n",
        "Install necessary Python libraries such as `torch` and `tiktoken`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a91336ce"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the `tiktoken` library using pip as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b475408",
        "outputId": "d81dd1f8-db6a-415d-d3c8-c3ed91ae2908"
      },
      "source": [
        "!pip install tiktoken"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5340a8b"
      },
      "source": [
        "**Reasoning**:\n",
        "Import torch and tiktoken to verify that they are successfully installed and ready for the subsequent tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbd5ff67",
        "outputId": "c2078511-9fc0-4b4f-8ca5-bc68818b0fe5"
      },
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(\"tiktoken imported successfully.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.9.0+cu128\n",
            "tiktoken imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75a015b7"
      },
      "source": [
        "# Task\n",
        "Implement and train a GPT language model from scratch on the TinyShakespeare dataset using PyTorch and tiktoken, including defining the Transformer architecture (Multi-Head Attention, FeedForward, Block) and generating sample text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b14910ed"
      },
      "source": [
        "## Download and Prepare Data\n",
        "\n",
        "### Subtask:\n",
        "Download the TinyShakespeare dataset, encode it using tiktoken, and split it into training and validation sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec9a25b7"
      },
      "source": [
        "**Reasoning**:\n",
        "Download the TinyShakespeare dataset, encode it using tiktoken, convert to tensors, and split into training and validation sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddabc52f",
        "outputId": "540023be-b2cf-4557-8ac4-fcc1b5420b87"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# 1. Download the dataset\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "file_path = 'input.txt'\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    data = requests.get(url).text\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write(data)\n",
        "\n",
        "# 2. Read contents\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"Length of dataset in characters: {len(text)}\")\n",
        "\n",
        "# 3. Initialize tiktoken encoder\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# 4. Encode the text\n",
        "ids = enc.encode(text)\n",
        "print(f\"Total tokens: {len(ids)}\")\n",
        "\n",
        "# 5. Convert to PyTorch tensor\n",
        "data_tensor = torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "# 6. Split into train and validation\n",
        "n = int(0.9 * len(data_tensor))\n",
        "train_data = data_tensor[:n]\n",
        "val_data = data_tensor[n:]\n",
        "\n",
        "# 7. Print stats\n",
        "vocab_size = enc.n_vocab\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Train set shape: {train_data.shape}\")\n",
        "print(f\"Validation set shape: {val_data.shape}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters: 1115394\n",
            "Total tokens: 338025\n",
            "Vocabulary size: 50257\n",
            "Train set shape: torch.Size([304222])\n",
            "Validation set shape: torch.Size([33803])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96c3917f"
      },
      "source": [
        "## Define Model Architecture\n",
        "\n",
        "### Subtask:\n",
        "Implement the GPT architecture including self-attention mechanisms, feed-forward networks, transformer blocks, and the main language model class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a125da04"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the GPT model architecture, including hyperparameters, data batching, and the Transformer components (Head, MultiHeadAttention, FeedForward, Block, GPTLanguageModel), as requested in the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29406fe6",
        "outputId": "8e7e2a95-e355-46de-fffd-0b595e27a015"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return self.dropout(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            Block(n_embd, n_head=n_head) for _ in range(n_layer)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.684497 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd456c79"
      },
      "source": [
        "## Train Model\n",
        "\n",
        "### Subtask:\n",
        "Create the training loop with the necessary hyperparameters and train the model on the prepared data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a28d53af"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the training loop, including loss estimation, optimizer initialization, and the iterative training process using the defined hyperparameters and model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56037356",
        "outputId": "f31b54be-48b5-41e1-c6cd-43a93935a9a7"
      },
      "source": [
        "# Create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    m.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = m(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    m.train()\n",
        "    return out\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 10.9982, val loss 11.0105\n",
            "step 100: train loss 6.3955, val loss 6.4657\n",
            "step 200: train loss 6.0255, val loss 6.1583\n",
            "step 300: train loss 5.7610, val loss 5.9578\n",
            "step 400: train loss 5.4810, val loss 5.7326\n",
            "step 500: train loss 5.2535, val loss 5.5390\n",
            "step 600: train loss 5.1008, val loss 5.3982\n",
            "step 700: train loss 4.9696, val loss 5.3369\n",
            "step 800: train loss 4.8841, val loss 5.2531\n",
            "step 900: train loss 4.8079, val loss 5.1672\n",
            "step 1000: train loss 4.7195, val loss 5.0769\n",
            "step 1100: train loss 4.6626, val loss 5.1190\n",
            "step 1200: train loss 4.5920, val loss 5.0147\n",
            "step 1300: train loss 4.5585, val loss 5.0320\n",
            "step 1400: train loss 4.4998, val loss 4.9957\n",
            "step 1500: train loss 4.4485, val loss 5.0001\n",
            "step 1600: train loss 4.4198, val loss 4.9252\n",
            "step 1700: train loss 4.4201, val loss 4.9456\n",
            "step 1800: train loss 4.3644, val loss 4.9211\n",
            "step 1900: train loss 4.3473, val loss 4.9224\n",
            "step 2000: train loss 4.2923, val loss 4.8641\n",
            "step 2100: train loss 4.2441, val loss 4.8797\n",
            "step 2200: train loss 4.2493, val loss 4.8732\n",
            "step 2300: train loss 4.1900, val loss 4.8295\n",
            "step 2400: train loss 4.1716, val loss 4.8688\n",
            "step 2500: train loss 4.1616, val loss 4.8803\n",
            "step 2600: train loss 4.1386, val loss 4.8906\n",
            "step 2700: train loss 4.1027, val loss 4.8540\n",
            "step 2800: train loss 4.1084, val loss 4.8739\n",
            "step 2900: train loss 4.0870, val loss 4.8577\n",
            "step 3000: train loss 4.0733, val loss 4.8350\n",
            "step 3100: train loss 4.0348, val loss 4.8596\n",
            "step 3200: train loss 4.0283, val loss 4.8661\n",
            "step 3300: train loss 3.9892, val loss 4.8832\n",
            "step 3400: train loss 3.9889, val loss 4.8571\n",
            "step 3500: train loss 3.9469, val loss 4.8599\n",
            "step 3600: train loss 3.9302, val loss 4.8518\n",
            "step 3700: train loss 3.9095, val loss 4.8320\n",
            "step 3800: train loss 3.9173, val loss 4.8908\n",
            "step 3900: train loss 3.9059, val loss 4.8688\n",
            "step 4000: train loss 3.8953, val loss 4.9035\n",
            "step 4100: train loss 3.8485, val loss 4.8676\n",
            "step 4200: train loss 3.8631, val loss 4.8930\n",
            "step 4300: train loss 3.7977, val loss 4.8492\n",
            "step 4400: train loss 3.8236, val loss 4.9492\n",
            "step 4500: train loss 3.8058, val loss 4.8732\n",
            "step 4600: train loss 3.7884, val loss 4.9161\n",
            "step 4700: train loss 3.8065, val loss 4.8440\n",
            "step 4800: train loss 3.7767, val loss 4.8568\n",
            "step 4900: train loss 3.7400, val loss 4.9256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80e0049a"
      },
      "source": [
        "# Task\n",
        "Generate 2000 tokens of text using the trained model starting from a zero context, decode the tokens using `tiktoken`, and print the generated output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0067da24"
      },
      "source": [
        "## Generate Text\n",
        "\n",
        "### Subtask:\n",
        "Generate 2000 tokens of text using the trained model starting from a zero context, decode the tokens using `tiktoken`, and print the generated output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c65d50c9"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate text using the trained model starting from a zero context, decode the tokens using `tiktoken`, and print the generated output as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5aff202",
        "outputId": "34b07929-e3e2-44a1-a07e-a8eaf2f31891"
      },
      "source": [
        "# Initialize the context with a single zero token\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "# Generate text\n",
        "# max_new_tokens=2000 to generate 2000 tokens\n",
        "generated_ids = m.generate(context, max_new_tokens=2000)\n",
        "\n",
        "# Decode the generated token IDs back to text\n",
        "generated_text = enc.decode(generated_ids[0].tolist())\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "! what any consul?\n",
            "\n",
            "RIVERS:\n",
            "I amain in that counsel is held:\n",
            "My gracious soul, or else he wakes?\n",
            "I'll gnue no anitude but bones;\n",
            "And that vir Rome shuns the next man be admitted.\n",
            "\n",
            "CAPULET:\n",
            "And now I shall my husband, CobUS:\n",
            "Though she is butcher'd at least.\n",
            "Was a plausible, I pent with a thousand access with cannot,\n",
            "But tips cold from the reverence ridon manner,\n",
            "And say adds with such tears, if I ne'er mercy in mocked what\n",
            "Takes this covertinius o' earth.\n",
            "\n",
            "BAPTISTA:\n",
            "Angelo, though now learn'd for stoop's hand,\n",
            "Like a little goddess see how comes; and my creditors are the sea\n",
            "answerish them,\n",
            "That thou do, that death, andvenants, nor the web\n",
            "Of my followers: are well but\n",
            "What may say was, by's sorrow,--\n",
            "Yet will gnament'd with a monster of the lips\n",
            "Of time cray treasure the wateryches.\n",
            "What say, which is the king?\n",
            "She lacks advised us?\n",
            "\n",
            "ESCALUS:\n",
            "As 'twixt Shore, my lord, Pompey,\n",
            "That dost thou likeness, betense with me,\n",
            "In music's chair, but by a though a worthy:'\n",
            "The one pleasant request, by gazing of stately tongue;\n",
            "Dowbrionmen and you more mercy\n",
            "More than as you and hearing in rogue,\n",
            "Ten repixt a thousand great commanding MOW' blackariousised;\n",
            "When first I do corrupt.\n",
            "\n",
            "First Senator:\n",
            "Petruchio, fair, you would do!\n",
            "\n",
            "COMINIUS:\n",
            "Madam, tell him, then.\n",
            "\n",
            "KING RICHARD III:\n",
            "O, I think we willingly enough;\n",
            "More than you prar Lodow'd in their hate in my tent:\n",
            "As for ourselves,\n",
            "Being Angelo, my ordinance before an aspect; condemned, like one;\n",
            "I would have been ten thousand security andal;\n",
            "The dust of syighs and his country,\n",
            "tis now, that honesty to lose the print.\n",
            "\n",
            "POMPEY:\n",
            "Indeed, stir you for our gentlefret Castle?\n",
            "Lo, his sister, Richard!\n",
            "But fair worthy comfort, appear; and very trick his valiant;\n",
            "And now 'tis a times at Dun rose.\n",
            "Marks cannot brook and Warwick,--\n",
            "\n",
            "LEONTES:\n",
            "That if thou hast so, didst pass hanged or let him plainly\n",
            "manly, but very as him to the custom of law.\n",
            "\n",
            "Provost:\n",
            "Per thee, look yourself!\n",
            "\n",
            "CORIOLANUS:\n",
            "But that\n",
            "His hour comesly for both--\n",
            "I must judge my solemn prince: a merry leisure say\n",
            "You seem to the pleasure; for some respect now, for more quiet.\n",
            "\n",
            "Second Servant:\n",
            "Hear my lord, twoly mast nickfranchenius\n",
            "change heads, that were that nay's fair.\n",
            "\n",
            "GLOUCESTER:\n",
            "My liege, which says my horse, that hearing alone.Engot\n",
            "God cry these homehip ducalion,\n",
            "Where is not this present horse than I have\n",
            "say our gracious take my name\n",
            "Thy gross? it was Death.\n",
            "\n",
            "LEONTES:\n",
            "And yet, urge it for, holdly robes,\n",
            "Shall have followed the death. Let nature Northumberland.\n",
            "\n",
            "GLOUCESTER:\n",
            "No, Tybalt's consider,\n",
            "I hate him thy brother to trouble how the court\n",
            "Am taken, as my friends, do it not.\n",
            "\n",
            "LEONTES:\n",
            "Sweet babe Warwick in his blood colts,\n",
            "They hither fellly in thinking,\n",
            "Twould word and myself to intercept the arm of all:\n",
            "Mine the spiritsness I spake not and direct,\n",
            "I'll send your high grace have your strange woman;\n",
            "Who ere thou rise good gentlemen, vizard,\n",
            "That--think'd thy noble man that small spake;\n",
            "There's thine and Sar quench.\n",
            "\n",
            "ROMEO:\n",
            "Who is how is content of that than my brother made\n",
            "Than you mind to answer any he had.\n",
            "Besides above, loath:\n",
            "Nor he were she bear it as quickly fairly:\n",
            "But we have kept mend with heed.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Not you, pardon I saw the man and that,\n",
            "Her pity come to London, and raise me in Padua.'\n",
            "Not 'tis,--thie, kill you at your fortune,\n",
            "For,\n",
            "being so eyes behind to rem in my earth.\n",
            "\n",
            "A Patrician, &CAS:\n",
            "I guess no. Fare yeuain,\n",
            "But if my soul's war is full of words,\n",
            "That we shall not play: throw away toMark.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "From spirits and says, and, are lords, O,\n",
            "You do homage and tell your mother.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "I wish him so as you were forfeit too.\n",
            "\n",
            "GLOUCESTER:\n",
            "But look you like harvest and bereft to please him:\n",
            "Nor I do not am sold without a queen,\n",
            "And urged to your honour, that way;\n",
            "And and in pomp'd grace ere you be found nor place\n",
            "For this teissest their ministers's law;\n",
            "it ere we would the queen's open,\n",
            "To see nothing begun shall lose the brideces shuns:\n",
            "The contents coy'd arms was lengthen'd; for the other senses\n",
            "eyestersow conn, and that hath rather physical\n",
            "himished'd withalament to his hatred,\n",
            "And Somerset like a t thousand timesatory got him,\n",
            "should and son thou thread woman dying lonely ellunder,\n",
            "But an Edward with fox success,\n",
            "And with whose soldiers', both\n",
            "I took your uncertainty, the ground, there\n",
            "To let his body's dev; and the malees,\n",
            "Till he will be acknowledged\n",
            "To weep, Hermione shall pay the stars being law,\n",
            "To show your children would Bolingbroke,\n",
            "And with our spacious up the own science.\n",
            "A parasite-browchief, and,\n",
            "Disble Paris; take them, and recreant me now\n",
            "Shall we like,, only told you would not get a dram pite.\n",
            "Take yourself his lovely rivers not a\n",
            "umpet take at his new four water\n",
            "s heaven. I am I count you thus.\n",
            "\n",
            "FLORIZEL:\n",
            "The next matter, I am at patience.\n",
            "\n",
            "MENENIUS:\n",
            "Come, good work, night; but they are part.\n",
            "\n",
            "CLARENCE:\n",
            "O villain, boy, make a goods makes heart.\n",
            "Truly, my condition since she alliance'd,\n",
            "A sin i' the rest were home,\n",
            "The was so dissemble, that would put you think,\n",
            "To depart cleoder Stafford, into no prophecy. Welcome, sir,\n",
            "Be hood King he thinks do put into,--\n",
            "But when a tall thousand noble driftop,\n",
            "Deckry and I wash me in my heels,\n",
            "With danger by plot all the drunk chamberic;\n",
            "Come hither on the sister that\n",
            "Are still. Myinking you.\n",
            "\n",
            "PRINCE:\n",
            "A gentler will\n",
            "My parasite, where I will, 'tis not keep:\n",
            "Hereep and buzzowbrusst set down,\n",
            "For his friends are convent on out\n",
            "Had not enter'd to wearose words,\n",
            "Where but abhorr'd his horns stones leave,\n",
            "Or another it not; that my life from France\n",
            "Untoebus dreamoa and battle,\n",
            "To give up the crown.\n",
            "\n",
            "LEONTES:\n",
            "From France.\n",
            "\n",
            "COMINIUS:\n",
            "But for if I think it at this,\n",
            "But pith requireed, nor lies,\n",
            "thick devotion deaf a cause. Sir William Brandon!\n",
            "\n",
            "KING EDWARD IV:\n",
            "My ownts with leads is both?\n",
            "\n",
            "JULIET:\n",
            "I am in despite of honour, sir?\n",
            " whose common justice,--M extremeous duty.\n",
            "\n",
            "TRANIO:\n",
            "My woman that says I silent Harry for remorse?\n",
            "\n",
            " JerENRY PERCY:\n",
            "But I'll Capulet nor greatness more strength\n",
            "Her nature wander'd as worship.\n",
            "\n",
            "ESCALUS:\n",
            "The exchangeeth are affected are yourselves\n",
            "And lasting unres companions Henry's law to hold\n",
            "And blowck their consists, to your spice for King.\n",
            "\n",
            "BAPTISTA:\n",
            "My Lord Angelo doth he rather stumble, I pray thee,\n",
            "The truth of her death, tender men departedAS use him:\n",
            "But in another hands to my subjects,\n",
            "Now must be not Lomb for their old,\n",
            "What, shoulder she is your great at faithful,\n",
            "Shall pay a tear in the west?\n",
            "\n",
            "Hostbot to the court your lip:\n",
            "I would be so look my thwart as his last,\n",
            "For promise thee, he looks from the clock,\n",
            "And broke having when thou art a saying Angelo,\n",
            "Through full, than often often put; and, if thou art mine;\n",
            "What, our dried his\n",
            "That all things inWith slow angry advocates.\n",
            "\n",
            "Servant:\n",
            "Why, do you break the great my good'd friends:\n",
            "An his folligh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09dcefb2"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the text generation results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfd58c28"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "**What is the output of the text generation model?**\n",
        "The model successfully generated 2000 tokens of text starting from a zero context. The output was decoded using `tiktoken` and printed, revealing text that mimics the structure and content of the training data.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The text generation process functioned correctly, producing a sequence of 2000 tokens derived from a `(1, 1)` zero-initialized tensor.\n",
        "*   The decoded text exhibits distinct characteristics of Shakespearean literature, indicating the model successfully learned patterns from the dataset.\n",
        "*   Specific entities and character names such as \"RIVERS\", \"CAPULET\", \"ROMEO\", and \"GLOUCESTER\" were identified in the output, demonstrating the model's ability to recall vocabulary specific to the training corpus.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The successful generation of stylistically relevant text suggests the model architecture and training process were effective for this dataset.\n",
        "*   A potential next step is to evaluate the model's performance quantitatively by calculating the loss on a validation set or qualitatively by providing specific text prompts to see how the model completes them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf123d47"
      },
      "source": [
        "## Qualitative Evaluation\n",
        "\n",
        "We can evaluate the model's performance qualitatively by providing specific starting prompts and observing how it completes the text. This checks if the model retains context and style given a specific seed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "969f8cd9",
        "outputId": "dc6a5e07-e351-410a-b32f-d241e5eec886"
      },
      "source": [
        "def generate_from_prompt(prompt_text, max_new_tokens=200):\n",
        "    # Encode the prompt\n",
        "    input_ids = enc.encode(prompt_text)\n",
        "    # Convert to tensor and add batch dimension (1, T)\n",
        "    context = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # Generate text\n",
        "    generated_ids = m.generate(context, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Decode and print\n",
        "    output_text = enc.decode(generated_ids[0].tolist())\n",
        "    print(f\"Prompt: '{prompt_text}'\")\n",
        "    print(\"-\" * 40)\n",
        "    print(output_text)\n",
        "    print(\"=\" * 40)\n",
        "    print()\n",
        "\n",
        "# Test with specific prompts\n",
        "test_prompts = [\n",
        "    \"ROMEO:\",\n",
        "    \"The king\",\n",
        "    \"To be, or not to be,\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    generate_from_prompt(prompt)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'ROMEO:'\n",
            "----------------------------------------\n",
            "ROMEO:\n",
            "No, that you for he had seen'd;\n",
            "The wrinkles chances of the lustbeokesome eye.\n",
            "\n",
            "SICINIUS:\n",
            "Renowned as well enough.\n",
            "\n",
            "ISABELLA:\n",
            "Then, here's no fighter:\n",
            "That's the king on him to London,\n",
            "That give yoked one that are:\n",
            "Stow two is my coat, and very pregnant,\n",
            "To make thee to our actions with grief.\n",
            "\n",
            "LEONTES:\n",
            "You have all three at goodartius:\n",
            "Neg much, we shall not grant how tor.\n",
            "\n",
            "KING HENRY VI:\n",
            "I'll keep my glass, hath drunkardener to himed\n",
            "In weighty, good for them, within it gone: the\n",
            "house mistress' love of Calais, this, I'll have\n",
            "past cool a favour.\n",
            "\n",
            "First Senator:\n",
            "I mean you.\n",
            "\n",
            "Captain:\n",
            "\n",
            "PERDITA:\n",
            "Away!\n",
            "========================================\n",
            "\n",
            "Prompt: 'The king'\n",
            "----------------------------------------\n",
            "The king with my soul's violence,\n",
            "Vaugh upon thy mother's forehead:\n",
            "O, my soul is thou dip'd, so mad and in here,\n",
            "Yet or comforts, go whilst I sent to withdraw\n",
            "I'ld order your bodiesches.\n",
            "We had might have come with thee;\n",
            "But how you are? Daughter concerns a bawdish at the gin of\n",
            "ENCE not, for the haste's death.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Madam, brother, Marcius, I will draw in the king is our\n",
            "That no sign-day, seeing befits acting,\n",
            "do he, by the volume-sheiser--\n",
            "But lean complexion o' the camp task at your heels,\n",
            "Then lie curse you adventure to thine Woodes.\n",
            "\n",
            "BUCKINGHAM:\n",
            "It makes your rue, God's child--\n",
            "My nurse, that be the bounds of a bloody tongue,\n",
            "Which next trodden world\n",
            "That give\n",
            "========================================\n",
            "\n",
            "Prompt: 'To be, or not to be,'\n",
            "----------------------------------------\n",
            "To be, or not to be, you then,\n",
            "Let stronger fall to see a bank's face.\n",
            "Now, wilt thou hast here'st to do you,\n",
            "I have a feasting ruinours of haste:\n",
            "So smile therefore had to take your devour\n",
            "propertbler and envy.\n",
            "\n",
            "ANGELO:\n",
            "Poor presence, he is joy of my hand.\n",
            "\n",
            "AllATCLIFFORD:\n",
            "I must die there, Clarence shall not.\n",
            "\n",
            "ISABELLA:\n",
            "I first: you were Proly.\n",
            "You do protest, boy.\n",
            "\n",
            "ROMEO:\n",
            "Whoever thought hath me but it with your blood.\n",
            "But what I did make a merry.\n",
            "O, belike so we do it\n",
            "Two, or never truly of the queen,\n",
            "Each fortune, hath paced to the roof, he hath done,\n",
            "to so fasting;\n",
            "Now'er your blood\n",
            "Engune thereby louder SAL your friends's edge.\n",
            "\n",
            "DUKE\n",
            "========================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}