{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMw+wGeCJvcZqTU/AQvWNxf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SephPalmer/learning-ai/blob/main/Kaparpathys_GPT_with_literature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgo6Jp-k5LL9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d9d3d7c"
      },
      "source": [
        "# Task\n",
        "Implement and train a GPT language model on the TinyShakespeare dataset using PyTorch and `tiktoken`, including data preparation, defining the Transformer architecture with Multi-Head Attention, and generating sample text after training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "992df8e4"
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "### Subtask:\n",
        "Install necessary Python libraries such as `torch` and `tiktoken`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a91336ce"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the `tiktoken` library using pip as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b475408",
        "outputId": "d81dd1f8-db6a-415d-d3c8-c3ed91ae2908"
      },
      "source": [
        "!pip install tiktoken"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5340a8b"
      },
      "source": [
        "**Reasoning**:\n",
        "Import torch and tiktoken to verify that they are successfully installed and ready for the subsequent tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbd5ff67",
        "outputId": "c2078511-9fc0-4b4f-8ca5-bc68818b0fe5"
      },
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(\"tiktoken imported successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.9.0+cu128\n",
            "tiktoken imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75a015b7"
      },
      "source": [
        "# Task\n",
        "Implement and train a GPT language model from scratch on the TinyShakespeare dataset using PyTorch and tiktoken, including defining the Transformer architecture (Multi-Head Attention, FeedForward, Block) and generating sample text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b14910ed"
      },
      "source": [
        "## Download and Prepare Data\n",
        "\n",
        "### Subtask:\n",
        "Download the TinyShakespeare dataset, encode it using tiktoken, and split it into training and validation sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec9a25b7"
      },
      "source": [
        "**Reasoning**:\n",
        "Download the TinyShakespeare dataset, encode it using tiktoken, convert to tensors, and split into training and validation sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddabc52f",
        "outputId": "540023be-b2cf-4557-8ac4-fcc1b5420b87"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# 1. Download the dataset\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "file_path = 'input.txt'\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    data = requests.get(url).text\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write(data)\n",
        "\n",
        "# 2. Read contents\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"Length of dataset in characters: {len(text)}\")\n",
        "\n",
        "# 3. Initialize tiktoken encoder\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# 4. Encode the text\n",
        "ids = enc.encode(text)\n",
        "print(f\"Total tokens: {len(ids)}\")\n",
        "\n",
        "# 5. Convert to PyTorch tensor\n",
        "data_tensor = torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "# 6. Split into train and validation\n",
        "n = int(0.9 * len(data_tensor))\n",
        "train_data = data_tensor[:n]\n",
        "val_data = data_tensor[n:]\n",
        "\n",
        "# 7. Print stats\n",
        "vocab_size = enc.n_vocab\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Train set shape: {train_data.shape}\")\n",
        "print(f\"Validation set shape: {val_data.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters: 1115394\n",
            "Total tokens: 338025\n",
            "Vocabulary size: 50257\n",
            "Train set shape: torch.Size([304222])\n",
            "Validation set shape: torch.Size([33803])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96c3917f"
      },
      "source": [
        "## Define Model Architecture\n",
        "\n",
        "### Subtask:\n",
        "Implement the GPT architecture including self-attention mechanisms, feed-forward networks, transformer blocks, and the main language model class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a125da04"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the GPT model architecture, including hyperparameters, data batching, and the Transformer components (Head, MultiHeadAttention, FeedForward, Block, GPTLanguageModel), as requested in the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29406fe6",
        "outputId": "8e7e2a95-e355-46de-fffd-0b595e27a015"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return self.dropout(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            Block(n_embd, n_head=n_head) for _ in range(n_layer)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.684497 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd456c79"
      },
      "source": [
        "## Train Model\n",
        "\n",
        "### Subtask:\n",
        "Create the training loop with the necessary hyperparameters and train the model on the prepared data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a28d53af"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the training loop, including loss estimation, optimizer initialization, and the iterative training process using the defined hyperparameters and model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56037356",
        "outputId": "f31b54be-48b5-41e1-c6cd-43a93935a9a7"
      },
      "source": [
        "# Create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    m.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = m(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    m.train()\n",
        "    return out\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 10.9982, val loss 11.0105\n",
            "step 100: train loss 6.3955, val loss 6.4657\n",
            "step 200: train loss 6.0255, val loss 6.1583\n",
            "step 300: train loss 5.7610, val loss 5.9578\n",
            "step 400: train loss 5.4810, val loss 5.7326\n",
            "step 500: train loss 5.2535, val loss 5.5390\n",
            "step 600: train loss 5.1008, val loss 5.3982\n",
            "step 700: train loss 4.9696, val loss 5.3369\n",
            "step 800: train loss 4.8841, val loss 5.2531\n",
            "step 900: train loss 4.8079, val loss 5.1672\n",
            "step 1000: train loss 4.7195, val loss 5.0769\n",
            "step 1100: train loss 4.6626, val loss 5.1190\n",
            "step 1200: train loss 4.5920, val loss 5.0147\n",
            "step 1300: train loss 4.5585, val loss 5.0320\n",
            "step 1400: train loss 4.4998, val loss 4.9957\n",
            "step 1500: train loss 4.4485, val loss 5.0001\n",
            "step 1600: train loss 4.4198, val loss 4.9252\n",
            "step 1700: train loss 4.4201, val loss 4.9456\n",
            "step 1800: train loss 4.3644, val loss 4.9211\n",
            "step 1900: train loss 4.3473, val loss 4.9224\n",
            "step 2000: train loss 4.2923, val loss 4.8641\n",
            "step 2100: train loss 4.2441, val loss 4.8797\n",
            "step 2200: train loss 4.2493, val loss 4.8732\n",
            "step 2300: train loss 4.1900, val loss 4.8295\n",
            "step 2400: train loss 4.1716, val loss 4.8688\n",
            "step 2500: train loss 4.1616, val loss 4.8803\n",
            "step 2600: train loss 4.1386, val loss 4.8906\n",
            "step 2700: train loss 4.1027, val loss 4.8540\n",
            "step 2800: train loss 4.1084, val loss 4.8739\n",
            "step 2900: train loss 4.0870, val loss 4.8577\n",
            "step 3000: train loss 4.0733, val loss 4.8350\n",
            "step 3100: train loss 4.0348, val loss 4.8596\n",
            "step 3200: train loss 4.0283, val loss 4.8661\n",
            "step 3300: train loss 3.9892, val loss 4.8832\n",
            "step 3400: train loss 3.9889, val loss 4.8571\n",
            "step 3500: train loss 3.9469, val loss 4.8599\n",
            "step 3600: train loss 3.9302, val loss 4.8518\n",
            "step 3700: train loss 3.9095, val loss 4.8320\n",
            "step 3800: train loss 3.9173, val loss 4.8908\n",
            "step 3900: train loss 3.9059, val loss 4.8688\n",
            "step 4000: train loss 3.8953, val loss 4.9035\n",
            "step 4100: train loss 3.8485, val loss 4.8676\n",
            "step 4200: train loss 3.8631, val loss 4.8930\n",
            "step 4300: train loss 3.7977, val loss 4.8492\n",
            "step 4400: train loss 3.8236, val loss 4.9492\n",
            "step 4500: train loss 3.8058, val loss 4.8732\n",
            "step 4600: train loss 3.7884, val loss 4.9161\n",
            "step 4700: train loss 3.8065, val loss 4.8440\n",
            "step 4800: train loss 3.7767, val loss 4.8568\n",
            "step 4900: train loss 3.7400, val loss 4.9256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80e0049a"
      },
      "source": [
        "# Task\n",
        "Generate 2000 tokens of text using the trained model starting from a zero context, decode the tokens using `tiktoken`, and print the generated output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0067da24"
      },
      "source": [
        "## Generate Text\n",
        "\n",
        "### Subtask:\n",
        "Generate 2000 tokens of text using the trained model starting from a zero context, decode the tokens using `tiktoken`, and print the generated output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c65d50c9"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate text using the trained model starting from a zero context, decode the tokens using `tiktoken`, and print the generated output as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5aff202",
        "outputId": "34b07929-e3e2-44a1-a07e-a8eaf2f31891"
      },
      "source": [
        "# Initialize the context with a single zero token\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "# Generate text\n",
        "# max_new_tokens=2000 to generate 2000 tokens\n",
        "generated_ids = m.generate(context, max_new_tokens=2000)\n",
        "\n",
        "# Decode the generated token IDs back to text\n",
        "generated_text = enc.decode(generated_ids[0].tolist())\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "! what any consul?\n",
            "\n",
            "RIVERS:\n",
            "I amain in that counsel is held:\n",
            "My gracious soul, or else he wakes?\n",
            "I'll gnue no anitude but bones;\n",
            "And that vir Rome shuns the next man be admitted.\n",
            "\n",
            "CAPULET:\n",
            "And now I shall my husband, CobUS:\n",
            "Though she is butcher'd at least.\n",
            "Was a plausible, I pent with a thousand access with cannot,\n",
            "But tips cold from the reverence ridon manner,\n",
            "And say adds with such tears, if I ne'er mercy in mocked what\n",
            "Takes this covertinius o' earth.\n",
            "\n",
            "BAPTISTA:\n",
            "Angelo, though now learn'd for stoop's hand,\n",
            "Like a little goddess see how comes; and my creditors are the sea\n",
            "answerish them,\n",
            "That thou do, that death, andvenants, nor the web\n",
            "Of my followers: are well but\n",
            "What may say was, by's sorrow,--\n",
            "Yet will gnament'd with a monster of the lips\n",
            "Of time cray treasure the wateryches.\n",
            "What say, which is the king?\n",
            "She lacks advised us?\n",
            "\n",
            "ESCALUS:\n",
            "As 'twixt Shore, my lord, Pompey,\n",
            "That dost thou likeness, betense with me,\n",
            "In music's chair, but by a though a worthy:'\n",
            "The one pleasant request, by gazing of stately tongue;\n",
            "Dowbrionmen and you more mercy\n",
            "More than as you and hearing in rogue,\n",
            "Ten repixt a thousand great commanding MOW' blackariousised;\n",
            "When first I do corrupt.\n",
            "\n",
            "First Senator:\n",
            "Petruchio, fair, you would do!\n",
            "\n",
            "COMINIUS:\n",
            "Madam, tell him, then.\n",
            "\n",
            "KING RICHARD III:\n",
            "O, I think we willingly enough;\n",
            "More than you prar Lodow'd in their hate in my tent:\n",
            "As for ourselves,\n",
            "Being Angelo, my ordinance before an aspect; condemned, like one;\n",
            "I would have been ten thousand security andal;\n",
            "The dust of syighs and his country,\n",
            "tis now, that honesty to lose the print.\n",
            "\n",
            "POMPEY:\n",
            "Indeed, stir you for our gentlefret Castle?\n",
            "Lo, his sister, Richard!\n",
            "But fair worthy comfort, appear; and very trick his valiant;\n",
            "And now 'tis a times at Dun rose.\n",
            "Marks cannot brook and Warwick,--\n",
            "\n",
            "LEONTES:\n",
            "That if thou hast so, didst pass hanged or let him plainly\n",
            "manly, but very as him to the custom of law.\n",
            "\n",
            "Provost:\n",
            "Per thee, look yourself!\n",
            "\n",
            "CORIOLANUS:\n",
            "But that\n",
            "His hour comesly for both--\n",
            "I must judge my solemn prince: a merry leisure say\n",
            "You seem to the pleasure; for some respect now, for more quiet.\n",
            "\n",
            "Second Servant:\n",
            "Hear my lord, twoly mast nickfranchenius\n",
            "change heads, that were that nay's fair.\n",
            "\n",
            "GLOUCESTER:\n",
            "My liege, which says my horse, that hearing alone.Engot\n",
            "God cry these homehip ducalion,\n",
            "Where is not this present horse than I have\n",
            "say our gracious take my name\n",
            "Thy gross? it was Death.\n",
            "\n",
            "LEONTES:\n",
            "And yet, urge it for, holdly robes,\n",
            "Shall have followed the death. Let nature Northumberland.\n",
            "\n",
            "GLOUCESTER:\n",
            "No, Tybalt's consider,\n",
            "I hate him thy brother to trouble how the court\n",
            "Am taken, as my friends, do it not.\n",
            "\n",
            "LEONTES:\n",
            "Sweet babe Warwick in his blood colts,\n",
            "They hither fellly in thinking,\n",
            "Twould word and myself to intercept the arm of all:\n",
            "Mine the spiritsness I spake not and direct,\n",
            "I'll send your high grace have your strange woman;\n",
            "Who ere thou rise good gentlemen, vizard,\n",
            "That--think'd thy noble man that small spake;\n",
            "There's thine and Sar quench.\n",
            "\n",
            "ROMEO:\n",
            "Who is how is content of that than my brother made\n",
            "Than you mind to answer any he had.\n",
            "Besides above, loath:\n",
            "Nor he were she bear it as quickly fairly:\n",
            "But we have kept mend with heed.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Not you, pardon I saw the man and that,\n",
            "Her pity come to London, and raise me in Padua.'\n",
            "Not 'tis,--thie, kill you at your fortune,\n",
            "For,\n",
            "being so eyes behind to rem in my earth.\n",
            "\n",
            "A Patrician, &CAS:\n",
            "I guess no. Fare yeuain,\n",
            "But if my soul's war is full of words,\n",
            "That we shall not play: throw away toMark.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "From spirits and says, and, are lords, O,\n",
            "You do homage and tell your mother.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "I wish him so as you were forfeit too.\n",
            "\n",
            "GLOUCESTER:\n",
            "But look you like harvest and bereft to please him:\n",
            "Nor I do not am sold without a queen,\n",
            "And urged to your honour, that way;\n",
            "And and in pomp'd grace ere you be found nor place\n",
            "For this teissest their ministers's law;\n",
            "it ere we would the queen's open,\n",
            "To see nothing begun shall lose the brideces shuns:\n",
            "The contents coy'd arms was lengthen'd; for the other senses\n",
            "eyestersow conn, and that hath rather physical\n",
            "himished'd withalament to his hatred,\n",
            "And Somerset like a t thousand timesatory got him,\n",
            "should and son thou thread woman dying lonely ellunder,\n",
            "But an Edward with fox success,\n",
            "And with whose soldiers', both\n",
            "I took your uncertainty, the ground, there\n",
            "To let his body's dev; and the malees,\n",
            "Till he will be acknowledged\n",
            "To weep, Hermione shall pay the stars being law,\n",
            "To show your children would Bolingbroke,\n",
            "And with our spacious up the own science.\n",
            "A parasite-browchief, and,\n",
            "Disble Paris; take them, and recreant me now\n",
            "Shall we like,, only told you would not get a dram pite.\n",
            "Take yourself his lovely rivers not a\n",
            "umpet take at his new four water\n",
            "s heaven. I am I count you thus.\n",
            "\n",
            "FLORIZEL:\n",
            "The next matter, I am at patience.\n",
            "\n",
            "MENENIUS:\n",
            "Come, good work, night; but they are part.\n",
            "\n",
            "CLARENCE:\n",
            "O villain, boy, make a goods makes heart.\n",
            "Truly, my condition since she alliance'd,\n",
            "A sin i' the rest were home,\n",
            "The was so dissemble, that would put you think,\n",
            "To depart cleoder Stafford, into no prophecy. Welcome, sir,\n",
            "Be hood King he thinks do put into,--\n",
            "But when a tall thousand noble driftop,\n",
            "Deckry and I wash me in my heels,\n",
            "With danger by plot all the drunk chamberic;\n",
            "Come hither on the sister that\n",
            "Are still. Myinking you.\n",
            "\n",
            "PRINCE:\n",
            "A gentler will\n",
            "My parasite, where I will, 'tis not keep:\n",
            "Hereep and buzzowbrusst set down,\n",
            "For his friends are convent on out\n",
            "Had not enter'd to wearose words,\n",
            "Where but abhorr'd his horns stones leave,\n",
            "Or another it not; that my life from France\n",
            "Untoebus dreamoa and battle,\n",
            "To give up the crown.\n",
            "\n",
            "LEONTES:\n",
            "From France.\n",
            "\n",
            "COMINIUS:\n",
            "But for if I think it at this,\n",
            "But pith requireed, nor lies,\n",
            "thick devotion deaf a cause. Sir William Brandon!\n",
            "\n",
            "KING EDWARD IV:\n",
            "My ownts with leads is both?\n",
            "\n",
            "JULIET:\n",
            "I am in despite of honour, sir?\n",
            " whose common justice,--M extremeous duty.\n",
            "\n",
            "TRANIO:\n",
            "My woman that says I silent Harry for remorse?\n",
            "\n",
            " JerENRY PERCY:\n",
            "But I'll Capulet nor greatness more strength\n",
            "Her nature wander'd as worship.\n",
            "\n",
            "ESCALUS:\n",
            "The exchangeeth are affected are yourselves\n",
            "And lasting unres companions Henry's law to hold\n",
            "And blowck their consists, to your spice for King.\n",
            "\n",
            "BAPTISTA:\n",
            "My Lord Angelo doth he rather stumble, I pray thee,\n",
            "The truth of her death, tender men departedAS use him:\n",
            "But in another hands to my subjects,\n",
            "Now must be not Lomb for their old,\n",
            "What, shoulder she is your great at faithful,\n",
            "Shall pay a tear in the west?\n",
            "\n",
            "Hostbot to the court your lip:\n",
            "I would be so look my thwart as his last,\n",
            "For promise thee, he looks from the clock,\n",
            "And broke having when thou art a saying Angelo,\n",
            "Through full, than often often put; and, if thou art mine;\n",
            "What, our dried his\n",
            "That all things inWith slow angry advocates.\n",
            "\n",
            "Servant:\n",
            "Why, do you break the great my good'd friends:\n",
            "An his folligh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09dcefb2"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the text generation results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfd58c28"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "**What is the output of the text generation model?**\n",
        "The model successfully generated 2000 tokens of text starting from a zero context. The output was decoded using `tiktoken` and printed, revealing text that mimics the structure and content of the training data.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The text generation process functioned correctly, producing a sequence of 2000 tokens derived from a `(1, 1)` zero-initialized tensor.\n",
        "*   The decoded text exhibits distinct characteristics of Shakespearean literature, indicating the model successfully learned patterns from the dataset.\n",
        "*   Specific entities and character names such as \"RIVERS\", \"CAPULET\", \"ROMEO\", and \"GLOUCESTER\" were identified in the output, demonstrating the model's ability to recall vocabulary specific to the training corpus.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The successful generation of stylistically relevant text suggests the model architecture and training process were effective for this dataset.\n",
        "*   A potential next step is to evaluate the model's performance quantitatively by calculating the loss on a validation set or qualitatively by providing specific text prompts to see how the model completes them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf123d47"
      },
      "source": [
        "## Qualitative Evaluation\n",
        "\n",
        "We can evaluate the model's performance qualitatively by providing specific starting prompts and observing how it completes the text. This checks if the model retains context and style given a specific seed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "969f8cd9",
        "outputId": "dc6a5e07-e351-410a-b32f-d241e5eec886"
      },
      "source": [
        "def generate_from_prompt(prompt_text, max_new_tokens=200):\n",
        "    # Encode the prompt\n",
        "    input_ids = enc.encode(prompt_text)\n",
        "    # Convert to tensor and add batch dimension (1, T)\n",
        "    context = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # Generate text\n",
        "    generated_ids = m.generate(context, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Decode and print\n",
        "    output_text = enc.decode(generated_ids[0].tolist())\n",
        "    print(f\"Prompt: '{prompt_text}'\")\n",
        "    print(\"-\" * 40)\n",
        "    print(output_text)\n",
        "    print(\"=\" * 40)\n",
        "    print()\n",
        "\n",
        "# Test with specific prompts\n",
        "test_prompts = [\n",
        "    \"ROMEO:\",\n",
        "    \"The king\",\n",
        "    \"To be, or not to be,\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    generate_from_prompt(prompt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'ROMEO:'\n",
            "----------------------------------------\n",
            "ROMEO:\n",
            "No, that you for he had seen'd;\n",
            "The wrinkles chances of the lustbeokesome eye.\n",
            "\n",
            "SICINIUS:\n",
            "Renowned as well enough.\n",
            "\n",
            "ISABELLA:\n",
            "Then, here's no fighter:\n",
            "That's the king on him to London,\n",
            "That give yoked one that are:\n",
            "Stow two is my coat, and very pregnant,\n",
            "To make thee to our actions with grief.\n",
            "\n",
            "LEONTES:\n",
            "You have all three at goodartius:\n",
            "Neg much, we shall not grant how tor.\n",
            "\n",
            "KING HENRY VI:\n",
            "I'll keep my glass, hath drunkardener to himed\n",
            "In weighty, good for them, within it gone: the\n",
            "house mistress' love of Calais, this, I'll have\n",
            "past cool a favour.\n",
            "\n",
            "First Senator:\n",
            "I mean you.\n",
            "\n",
            "Captain:\n",
            "\n",
            "PERDITA:\n",
            "Away!\n",
            "========================================\n",
            "\n",
            "Prompt: 'The king'\n",
            "----------------------------------------\n",
            "The king with my soul's violence,\n",
            "Vaugh upon thy mother's forehead:\n",
            "O, my soul is thou dip'd, so mad and in here,\n",
            "Yet or comforts, go whilst I sent to withdraw\n",
            "I'ld order your bodiesches.\n",
            "We had might have come with thee;\n",
            "But how you are? Daughter concerns a bawdish at the gin of\n",
            "ENCE not, for the haste's death.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Madam, brother, Marcius, I will draw in the king is our\n",
            "That no sign-day, seeing befits acting,\n",
            "do he, by the volume-sheiser--\n",
            "But lean complexion o' the camp task at your heels,\n",
            "Then lie curse you adventure to thine Woodes.\n",
            "\n",
            "BUCKINGHAM:\n",
            "It makes your rue, God's child--\n",
            "My nurse, that be the bounds of a bloody tongue,\n",
            "Which next trodden world\n",
            "That give\n",
            "========================================\n",
            "\n",
            "Prompt: 'To be, or not to be,'\n",
            "----------------------------------------\n",
            "To be, or not to be, you then,\n",
            "Let stronger fall to see a bank's face.\n",
            "Now, wilt thou hast here'st to do you,\n",
            "I have a feasting ruinours of haste:\n",
            "So smile therefore had to take your devour\n",
            "propertbler and envy.\n",
            "\n",
            "ANGELO:\n",
            "Poor presence, he is joy of my hand.\n",
            "\n",
            "AllATCLIFFORD:\n",
            "I must die there, Clarence shall not.\n",
            "\n",
            "ISABELLA:\n",
            "I first: you were Proly.\n",
            "You do protest, boy.\n",
            "\n",
            "ROMEO:\n",
            "Whoever thought hath me but it with your blood.\n",
            "But what I did make a merry.\n",
            "O, belike so we do it\n",
            "Two, or never truly of the queen,\n",
            "Each fortune, hath paced to the roof, he hath done,\n",
            "to so fasting;\n",
            "Now'er your blood\n",
            "Engune thereby louder SAL your friends's edge.\n",
            "\n",
            "DUKE\n",
            "========================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "210f43da"
      },
      "source": [
        "# Task\n",
        "Implement a data loading step using a `USE_LARGE_DATASET` flag. If `True`, download and combine the texts from \"https://www.gutenberg.org/files/1342/1342-0.txt\", \"https://www.gutenberg.org/files/11/11-0.txt\", and \"https://www.gutenberg.org/files/84/84-0.txt\"; otherwise, use the existing TinyShakespeare dataset. Encode the selected text using `tiktoken` and split it into training (90%) and validation (10%) sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1e5bc34"
      },
      "source": [
        "## Select and Prepare Dataset\n",
        "\n",
        "### Subtask:\n",
        "Implement data loading with a `USE_LARGE_DATASET` flag to choose between a combined large dataset or the original TinyShakespeare, then encode and split the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce35342e"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the logic to select between a large dataset (downloaded from Gutenberg) and the existing TinyShakespeare dataset based on a flag, then encode and split the data as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b77fe918",
        "outputId": "22e5b531-9019-44db-ece7-8a21bde6d2f7"
      },
      "source": [
        "import requests\n",
        "import torch\n",
        "\n",
        "# 1. Define flag\n",
        "USE_LARGE_DATASET = True\n",
        "\n",
        "# 2. List of URLs\n",
        "urls = [\n",
        "    \"https://www.gutenberg.org/files/1342/1342-0.txt\",\n",
        "    \"https://www.gutenberg.org/files/11/11-0.txt\",\n",
        "    \"https://www.gutenberg.org/files/84/84-0.txt\"\n",
        "]\n",
        "\n",
        "# 3. Load data based on flag\n",
        "if USE_LARGE_DATASET:\n",
        "    text = \"\"\n",
        "    print(\"Downloading and combining large dataset...\")\n",
        "    for url in urls:\n",
        "        try:\n",
        "            r = requests.get(url)\n",
        "            r.encoding = 'utf-8' # Ensure correct encoding\n",
        "            text += r.text\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading {url}: {e}\")\n",
        "else:\n",
        "    with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "print(f\"Length of dataset in characters: {len(text)}\")\n",
        "\n",
        "# 4. Encode using existing encoder 'enc'\n",
        "# Note: 'enc' assumes tiktoken encoder is already defined in previous cells\n",
        "ids = enc.encode(text)\n",
        "print(f\"Total tokens: {len(ids)}\")\n",
        "\n",
        "# 5. Convert to PyTorch tensor\n",
        "data_tensor = torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "# 6. Calculate split index\n",
        "n = int(0.9 * len(data_tensor))\n",
        "\n",
        "# 7. Split into train and validation\n",
        "train_data = data_tensor[:n]\n",
        "val_data = data_tensor[n:]\n",
        "\n",
        "# 8. Print stats\n",
        "print(f\"Train set shape: {train_data.shape}\")\n",
        "print(f\"Validation set shape: {val_data.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and combining large dataset...\n",
            "Length of dataset in characters: 1292976\n",
            "Total tokens: 337980\n",
            "Train set shape: torch.Size([304182])\n",
            "Validation set shape: torch.Size([33798])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbb224b2"
      },
      "source": [
        "## Re-initialize Model\n",
        "\n",
        "### Subtask:\n",
        "Re-instantiate the `GPTLanguageModel` to reset the weights and prepare for training on the new dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78b44427"
      },
      "source": [
        "**Reasoning**:\n",
        "Re-initialize the GPTLanguageModel to reset weights and move it to the configured device for the new training task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36d15fed",
        "outputId": "e5abdb67-6b9b-465d-b5fb-6ab8941dae70"
      },
      "source": [
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.684497 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b0eab73"
      },
      "source": [
        "# Task\n",
        "Train the re-initialized GPT model on the newly prepared large dataset. Create a new `AdamW` optimizer for the new model parameters and run the training loop for `max_iters` iterations, printing the training and validation loss every `eval_interval` steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4832b205"
      },
      "source": [
        "## Train Model\n",
        "\n",
        "### Subtask:\n",
        "Train the re-initialized model on the large dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbf4eb65"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the training loop for the re-initialized model. This involves creating a new optimizer and iterating through the training steps, periodically evaluating and printing the loss on the new large dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "647c0ac3",
        "outputId": "c7e82447-ab72-46a9-b6c2-34f6d9bc2ffd"
      },
      "source": [
        "# Create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 10.9944, val loss 10.9822\n",
            "step 100: train loss 6.4887, val loss 7.0498\n",
            "step 200: train loss 6.0514, val loss 6.7228\n",
            "step 300: train loss 5.7021, val loss 6.4811\n",
            "step 400: train loss 5.4908, val loss 6.3180\n",
            "step 500: train loss 5.3937, val loss 6.2144\n",
            "step 600: train loss 5.2477, val loss 6.1225\n",
            "step 700: train loss 5.1343, val loss 6.0375\n",
            "step 800: train loss 5.0210, val loss 6.0017\n",
            "step 900: train loss 4.9366, val loss 5.9344\n",
            "step 1000: train loss 4.8617, val loss 5.9124\n",
            "step 1100: train loss 4.8181, val loss 5.8430\n",
            "step 1200: train loss 4.7529, val loss 5.8345\n",
            "step 1300: train loss 4.7123, val loss 5.8305\n",
            "step 1400: train loss 4.6222, val loss 5.7991\n",
            "step 1500: train loss 4.6129, val loss 5.7470\n",
            "step 1600: train loss 4.5740, val loss 5.7676\n",
            "step 1700: train loss 4.4885, val loss 5.7117\n",
            "step 1800: train loss 4.4244, val loss 5.7271\n",
            "step 1900: train loss 4.4339, val loss 5.7065\n",
            "step 2000: train loss 4.4067, val loss 5.6900\n",
            "step 2100: train loss 4.3640, val loss 5.6576\n",
            "step 2200: train loss 4.3325, val loss 5.6274\n",
            "step 2300: train loss 4.3144, val loss 5.6381\n",
            "step 2400: train loss 4.2714, val loss 5.6432\n",
            "step 2500: train loss 4.2465, val loss 5.6154\n",
            "step 2600: train loss 4.2046, val loss 5.6482\n",
            "step 2700: train loss 4.1980, val loss 5.6292\n",
            "step 2800: train loss 4.1570, val loss 5.5856\n",
            "step 2900: train loss 4.1159, val loss 5.6485\n",
            "step 3000: train loss 4.1048, val loss 5.6364\n",
            "step 3100: train loss 4.0837, val loss 5.6362\n",
            "step 3200: train loss 4.0665, val loss 5.6293\n",
            "step 3300: train loss 4.0552, val loss 5.6288\n",
            "step 3400: train loss 4.0361, val loss 5.5908\n",
            "step 3500: train loss 4.0029, val loss 5.5812\n",
            "step 3600: train loss 3.9790, val loss 5.5868\n",
            "step 3700: train loss 3.9662, val loss 5.5901\n",
            "step 3800: train loss 3.9786, val loss 5.6134\n",
            "step 3900: train loss 3.9136, val loss 5.6147\n",
            "step 4000: train loss 3.9163, val loss 5.6040\n",
            "step 4100: train loss 3.9231, val loss 5.6299\n",
            "step 4200: train loss 3.9097, val loss 5.6426\n",
            "step 4300: train loss 3.8948, val loss 5.6039\n",
            "step 4400: train loss 3.8620, val loss 5.6126\n",
            "step 4500: train loss 3.8657, val loss 5.6020\n",
            "step 4600: train loss 3.8697, val loss 5.6181\n",
            "step 4700: train loss 3.8179, val loss 5.6192\n",
            "step 4800: train loss 3.8031, val loss 5.6470\n",
            "step 4900: train loss 3.7952, val loss 5.6289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a24fdc7"
      },
      "source": [
        "# Task\n",
        "Generate 2000 tokens of text using the re-trained model starting from a zero context, decode the tokens using `tiktoken`, and print the output. Additionally, generate text starting with the specific prompts \"ROMEO:\", \"The king\", and \"To be, or not to be,\" to observe the stylistic differences in the new model's output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "179b110d"
      },
      "source": [
        "## Generate Text\n",
        "\n",
        "### Subtask:\n",
        "Generate 2000 tokens from zero context and test specific prompts with the re-trained model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa5f55e8"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate 2000 tokens from a zero context using the re-trained model and test specific prompts to evaluate the model's performance on the new dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aa8c886",
        "outputId": "dff62267-f51e-46a7-e913-d004be4d9485"
      },
      "source": [
        "# Initialize the context with a single zero token\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "# Generate text\n",
        "print(\"Generating 2000 tokens from zero context...\")\n",
        "generated_ids = m.generate(context, max_new_tokens=2000)\n",
        "\n",
        "# Decode the generated token IDs back to text\n",
        "generated_text = enc.decode(generated_ids[0].tolist())\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test with specific prompts using the previously defined function\n",
        "new_test_prompts = [\n",
        "    \"ROMEO:\",\n",
        "    \"The king\",\n",
        "    \"To be, or not to be,\"\n",
        "]\n",
        "\n",
        "print(\"Testing specific prompts...\")\n",
        "for prompt in new_test_prompts:\n",
        "    generate_from_prompt(prompt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 2000 tokens from zero context...\n",
            "!\n",
            "\n",
            "Yous madam, said he, I do not like, then I Philips!they\n",
            "I do not what I should believe I were make to obey offending that day, but by many\n",
            "himself. These are not share of money I call myself bybearing to be\n",
            "over.\n",
            "\n",
            "He has happened long claws, it is not plainly ordered of the\n",
            "town, they cannot describe, indeed.\n",
            "\n",
            "I, read to confirm what you were already only do? thought\n",
            "said, cried, I have certainlyt even it. said Elizabeth, than usual.\n",
            "\n",
            "\n",
            "Chapter 15\n",
            "\n",
            "\n",
            "CHAPTERension.\n",
            "Advad as day was neither tongues him my collection dear, shrieks had\n",
            "only pleased from Mr. Bingley was in spite of my footfavour. My father is\n",
            "requiculate, is you discovered by admiration. A h suspicion of remorse\n",
            "left it is known to you; for it?\n",
            "\n",
            "Yes, I know that am false other customs greatly, indeed, IDo you join. Here really\n",
            "That wasnt know that! Let me mend, I shall, trust it, replied his side, acted and\n",
            "at your mother, they were acquainted with. I might have anything without him,\n",
            "ill handsome to make such as it aside. My aunt will like your, but nothing\n",
            "elevilyitty._able as these hope else is Mr. Collins,--but to be the\n",
            "result of him a person of talking to you would not venture to meet with me to deserve to hope,\n",
            "and certainly would not to wish it to be; and that it does there is perfectly\n",
            "to make her sisters.\n",
            "\n",
            "Oh, you must have him or asking me with so odd of what I am afraid you\n",
            "quite, and must have yet any more well blasted a meaning for the difference\n",
            "way of the way the streets.\n",
            "\n",
            "She was talking to undertake to drink, I have picturedront to me complete my\n",
            "friend.\n",
            "\n",
            "_were also! we dependence, the tallest! greatly are so _it, cried her head to go\n",
            "him: shall soum through you.\n",
            "\n",
            "His are a witness connected, said I to whom I could speaker turned to you decline\n",
            "pvent give my arrival before I had come? How\n",
            "what claim? What was my ability. Mr. Bingleys thoughts, Ill\n",
            "_all_ tired of an acquaintance, is so wholly you shall you who all can\n",
            "at least than I may spare the water about somented as you.\n",
            "\n",
            "Yes, he chose to think when it, Of you should marry? on? Do\n",
            "either the Rabbit eatasure?\n",
            "\n",
            "A secret is not, restoredfully, how read, said he, for the Queen, information out\n",
            "common forbid. He has I really, and beg that sort of such a\n",
            "engagements; and I did you all! I do not wish it paper with him to which\n",
            "excuse me shall hardly feel even say my confidence a elegance and fate.s\n",
            "hopingley was a short with being a noise of affected as Mr. Darcy\n",
            "kinder-and-establish! she was supposed no right of her companion,\n",
            "and ran to began frown from being; awake in the children, in a\n",
            "dressed the corps; but Mrs. Bennet was dict information necessary to the\n",
            "engagements on finding each folly, and burying a more idea of where she had herself during\n",
            "respect the restyouth themselves. Fitzwilliam, as it must will pity.\n",
            "\n",
            "Mrs. Bennet was now friend, kissed really she looked down in a most wished to\n",
            "Wickham and aunt; and Elizabeth walked a little fund into a great\n",
            "proves well still rendered various_ alive to leave to say Safie here.\n",
            "\n",
            "Morning, however, when once had something of time well as she entered herself,\n",
            "she continued to be drivenance again, but soon as he chose to get how\n",
            "other, would, she cared what round them all went away.\n",
            "\n",
            "ootifulAlice was humbled, and then burst up and others was proflig. He was\n",
            "seemedted by her to observe a to what of his increasing form as soon came his\n",
            "ception; but herself that he really said to carelessly in water.\n",
            "\n",
            "You were strange and down without sleepy for your daughter a bit, so there are something to another out\n",
            "mortal intending to ridicule for you purpose to be seen; but\n",
            "hopes ourIIIting it will be in town.\n",
            "\n",
            "Elizabeth is not too well Deply to hesitate for her age with it out\n",
            "power?though so so the very pursuit of guests all the possibility of indispensable,\n",
            "either, which he could reserve, when they had placed all the officers.\n",
            "\n",
            "But too it is that sort of deep and remarkable, what was in my\n",
            "water struck instances, I should have! But a fortnight.\n",
            "\n",
            "Thisade my fire was on, saying and assumed of which uncertainty shan\n",
            "concluded as he had forget, at all here. Sir William, my dear,\n",
            "regard us, I shouldn will allow, get with me?\n",
            "\n",
            "Just you can ever as good, said Elizabeth in essentials, turned, so very civil, I can be to me\n",
            "that ever? and the report. I\n",
            "does! Do you deserve nothing on so soon all that, is not absolutely at _that_ a\n",
            "ves so impatient.\n",
            "\n",
            "Here wandering, as gave her having pleasure if not of life was to give him to\n",
            "the kind of it, almost all that he should not believe it to your\n",
            "history of poverty with _some_ are_ like, and in all restraint are about less married,\n",
            "and if you will do all his the greatest girl the leaves had liked of those\n",
            "grav or often, I am profeness for those five without bounds? in the\n",
            "presentmen of music of what do? Cursed change, frequently are as well known of success,\n",
            "I should meet, be ill, only by none.\n",
            "\n",
            "They were assured to speak mentioned. when I expressed his friend were deeply dupl\n",
            "husband.\n",
            "\n",
            "From all the executioner eyes of time. On some eagerly that time I could boast\n",
            "procure my unhappiness, for it was happy necessity in love; in my absence can\n",
            "be my laboratory.\n",
            "\n",
            "Indeed, however, you must wait of doubt on you, I would not expect he hope,\n",
            "       having rather? But that if he added this is not hair. We are it so little\n",
            "    . I beg your manifold, who is grappling with, Miss Lucas _never_._ Lucas\n",
            "      might be become, tutlingness, and read it will carry no nastyably\n",
            "         on Sunday, if you like anything tougher pounds again? When I\n",
            "     engagement assured it as you were a very humbled.\n",
            "\n",
            "\n",
            "[Illustration: It like it there\n",
            "\n",
            "Alice know the Gryphon the Dormouse, torn in her air Can we vanished-desancing!\n",
            "gloom!even I really is evidently to be happy feelings to make the fifth of\n",
            "danger, but I strikeassure again; you will all the cause of it is\n",
            "some; and I am over Charlotte directly, I remember from this period to receive\n",
            "the sccip.\n",
            "\n",
            "It is soon as much. Mr. Darcy we have me crossed it?\n",
            "\n",
            "Who is so to put it, said the Cat, I should! my answer,\n",
            "piling.\n",
            "\n",
            "At you, she quite replied, about it as she estate. Well! U very\n",
            "nowend! thus must kindness_I_ go really a mad right. I itll experienced\n",
            "fuously. And I suppose to write moment it, hereing to all the\n",
            "variety of it--about that decease all myself and asking\n",
            "a slightable it: _his_, was a right for_, about the _you_ (_ presents\n",
            "shape, for you had right, even after me, then the end whom I\n",
            "making I thought that I am so gross. None with penetrate from France one\n",
            "cended, as to be of seeing too; and I shall not talk to see that his\n",
            "eyes can we have left night.\n",
            "\n",
            "Elizabeth had to readilyPri them all my enemies, the sun which I shall\n",
            "hard but attended to do not direct yourself about\n",
            "a to be passed. One is, and will you make _German_ a proverb,\n",
            "_changed. And he must soon Alice.\n",
            "\n",
            "To be rightly afraid I have now told me to see that the dungeon I had looked\n",
            "at food. They are not. One cannot describe to do not believe in\n",
            "you wish, and depend upon a cause or seasoncious\n",
            "================================================================================\n",
            "Testing specific prompts...\n",
            "Prompt: 'ROMEO:'\n",
            "----------------------------------------\n",
            "ROMEO: I\n",
            "thinkt knowisitive down here! cried Elizabeth,--the March Hare!\n",
            " dange sauceortling. Let one!--t try! Do you? Are they see\n",
            "what as they hear it! Thanks, make Mr. Collins?\n",
            "\n",
            "Elizabeth went on some Alice, Led! _he_s in_ again.\n",
            "\n",
            "This poured! said the Mouse, for to your uncle\n",
            "daughter, my being, I may know it.\n",
            "\n",
            "Which heard them all this entreaties represented passed and their company before to\n",
            "these execution respect were directed to me. The lovely voyage of renew and\n",
            "the hearts open the village of his companions could elsewhere a self-hole\n",
            "once inclination, just as well received murder. Our author yes!\n",
            "\n",
            "(s more than the next sentence\n",
            "that manner of an ladys form, that\n",
            "========================================\n",
            "\n",
            "Prompt: 'The king'\n",
            "----------------------------------------\n",
            "The king, who led a boy much less visitor in\n",
            "is confusion, withre early, standing at her mothers conduct of\n",
            "her. I would not have heard him when, and after his successor, with cold\n",
            "in fine felicity. I remained, but how delineigzag.\n",
            "\n",
            "Indeed, cried Elizabeth, has hears me. cried Maria, in a more civil\n",
            "of severalydia isaring that such a object of deepy, a\n",
            "woman have encouraged to see Mr. Bingleys curiosity call on, she does not so\n",
            "the long parting of her laughing as they were now as possible. The second author\n",
            "of her daughter companion inquired on Miss Bingley, where only was therefore likewise\n",
            "about to hope. They saw in all.\n",
            "But how to write, was why sentence have an unfortunate,\n",
            "oop it most acutelym afraid to love, as the only creature of the\n",
            "sense of succeeding acquaintance\n",
            "========================================\n",
            "\n",
            "Prompt: 'To be, or not to be,'\n",
            "----------------------------------------\n",
            "To be, or not to be,\n",
            "the right heart struck you had not gone all pard yesterday. My courage did, life! And\n",
            "something is your head?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER XXVIII.\n",
            "\n",
            "\n",
            "[Illustration]\n",
            "\n",
            "As, in spite of goodness, the air which she relatedter for the Gryformed\n",
            "calling it to the door and speedI did not begin; to deceive my appetite,\n",
            "the night the valley recofs; but to Geneva, in the same\n",
            "jury-natured, I cannot see very grave for a fortnight than I should not understand.\n",
            "\n",
            "Jane could, very soon pulled, sarcastic arranged, with his return, and as she was over\n",
            "judgeured when she spoke convinced by an hour for herself in the ground had\n",
            "judges the fair time of his hand in other side\n",
            " anger.\n",
            "\n",
            "Oh, you know. said you cut it Young, I am not as\n",
            "\n",
            "========================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "531c7362"
      },
      "source": [
        "## Summary of Results\n",
        "\n",
        "### Text Generation\n",
        "The model trained on the large dataset (likely a combination of novels like *Pride and Prejudice*, *Alice's Adventures in Wonderland*, and *Frankenstein* based on the character names and context) produced text that blends these distinct styles.\n",
        "\n",
        "*   **Zero Context Generation:** The output features characters like Elizabeth, Mr. Bingley, Mrs. Bennet, Alice, and the March Hare. The style fluctuates between the social commentary of Austen and the surrealism of Carroll.\n",
        "*   **Specific Prompts:**\n",
        "    *   **\"ROMEO:\":** Instead of Shakespearean dialogue, the model generated text involving Elizabeth and the March Hare, indicating the Shakespearean influence is absent or overwhelmed by the new dataset.\n",
        "    *   **\"The king\":** Transitioned into narrative text involving Elizabeth and Mr. Bingley.\n",
        "    *   **\"To be, or not to be,\":** Did not trigger a Shakespearean soliloquy but rather a narrative segment involving Jane and Geneva.\n",
        "\n",
        "This confirms that the model successfully learned from the new dataset and its behavior shifted accordingly, no longer producing Shakespearean text."
      ]
    }
  ]
}