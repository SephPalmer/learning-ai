{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SephPalmer/learning-ai/blob/main/Kaparpathys_GPT_web_source_769.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgo6Jp-k5LL9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d9d3d7c"
      },
      "source": [
        "# Task\n",
        "Implement and train a GPT language model on the TinyShakespeare dataset using PyTorch and `tiktoken`, including data preparation, defining the Transformer architecture with Multi-Head Attention, and generating sample text after training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "992df8e4"
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "### Subtask:\n",
        "Install necessary Python libraries such as `torch` and `tiktoken`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a91336ce"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the `tiktoken` library using pip as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b475408",
        "outputId": "b9cdcf41-8252-4484-c666-4c03674b3d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5340a8b"
      },
      "source": [
        "**Reasoning**:\n",
        "Import torch and tiktoken to verify that they are successfully installed and ready for the subsequent tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbd5ff67",
        "outputId": "06051be2-cf3e-43a5-e6c6-689d5c9f6b6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 2.10.0+cu128\n",
            "tiktoken imported successfully.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(\"tiktoken imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75a015b7"
      },
      "source": [
        "# Task\n",
        "Implement and train a GPT language model from scratch on the TinyShakespeare dataset using PyTorch and tiktoken, including defining the Transformer architecture (Multi-Head Attention, FeedForward, Block) and generating sample text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b14910ed"
      },
      "source": [
        "## Download and Prepare Data\n",
        "\n",
        "### Subtask:\n",
        "Download the TinyShakespeare dataset, encode it using tiktoken, and split it into training and validation sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec9a25b7"
      },
      "source": [
        "**Reasoning**:\n",
        "Download the TinyShakespeare dataset, encode it using tiktoken, convert to tensors, and split into training and validation sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddabc52f",
        "outputId": "0f77b89b-5044-4278-990a-49f8a9c526f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from c4-train.00000-of-01024.json.gz...\n",
            "Length of dataset in characters: 769299382\n",
            "Total tokens: 170592661\n",
            "Train set shape: torch.Size([153533394])\n",
            "Validation set shape: torch.Size([17059267])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tiktoken\n",
        "import gzip\n",
        "import json\n",
        "\n",
        "# Load Hugging Face Data (C4) from local file\n",
        "file_path = \"c4-train.00000-of-01024.json.gz\"\n",
        "print(f\"Loading data from {file_path}...\")\n",
        "\n",
        "#TARGET_CHARS = 20_000_000 # Limit to 20M characters for this run\n",
        "\n",
        "text = \"\"\n",
        "with gzip.open(file_path, 'rb') as f:\n",
        "    raw = f.read()\n",
        "\n",
        "records = [json.loads(line) for line in raw.decode('utf-8').splitlines()]\n",
        "text = \"\\n\".join(r[\"text\"] for r in records)\n",
        "print(f\"Length of dataset in characters: {len(text)}\")\n",
        "\n",
        "# Initialize tiktoken encoder\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = enc.n_vocab\n",
        "\n",
        "# Encode the text\n",
        "ids = enc.encode(text)\n",
        "print(f\"Total tokens: {len(ids)}\")\n",
        "\n",
        "# Convert to PyTorch tensor\n",
        "data_tensor = torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "# Split into train and validation\n",
        "n = int(0.9 * len(data_tensor))\n",
        "train_data = data_tensor[:n]\n",
        "val_data = data_tensor[n:]\n",
        "\n",
        "print(f\"Train set shape: {train_data.shape}\")\n",
        "print(f\"Validation set shape: {val_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96c3917f"
      },
      "source": [
        "## Define Model Architecture\n",
        "\n",
        "### Subtask:\n",
        "Implement the GPT architecture including self-attention mechanisms, feed-forward networks, transformer blocks, and the main language model class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a125da04"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the GPT model architecture, including hyperparameters, data batching, and the Transformer components (Head, MultiHeadAttention, FeedForward, Block, GPTLanguageModel), as requested in the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29406fe6",
        "outputId": "1f072511-7dff-4182-fe85-69ba0a8bfa29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "83.285073 M parameters\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 96\n",
        "block_size = 512\n",
        "max_iters = 25000\n",
        "eval_interval = 100\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 512\n",
        "n_head = 8\n",
        "n_layer = 10\n",
        "dropout = 0.1\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return self.dropout(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            Block(n_embd, n_head=n_head) for _ in range(n_layer)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd456c79"
      },
      "source": [
        "## Train Model\n",
        "\n",
        "### Subtask:\n",
        "Create the training loop with the necessary hyperparameters and train the model on the prepared data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a28d53af"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the training loop, including loss estimation, optimizer initialization, and the iterative training process using the defined hyperparameters and model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "56037356",
        "outputId": "eae518d7-0a23-4c63-f9ab-8064dfec2955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: training data loss 10.9894, loss against previously unseen data 10.9890\n",
            "step 100: training data loss 7.0105, loss against previously unseen data 7.0331\n",
            "step 200: training data loss 6.6703, loss against previously unseen data 6.6941\n",
            "step 300: training data loss 6.4883, loss against previously unseen data 6.5081\n",
            "step 400: training data loss 6.3196, loss against previously unseen data 6.3354\n",
            "step 500: training data loss 6.1988, loss against previously unseen data 6.2118\n",
            "step 600: training data loss 6.0811, loss against previously unseen data 6.1072\n",
            "step 700: training data loss 5.9782, loss against previously unseen data 6.0034\n",
            "step 800: training data loss 5.8778, loss against previously unseen data 5.9086\n",
            "step 900: training data loss 5.7864, loss against previously unseen data 5.8185\n",
            "step 1000: training data loss 5.7165, loss against previously unseen data 5.7455\n",
            "step 1100: training data loss 5.6348, loss against previously unseen data 5.6730\n",
            "step 1200: training data loss 5.5733, loss against previously unseen data 5.6082\n",
            "step 1300: training data loss 5.5122, loss against previously unseen data 5.5539\n",
            "step 1400: training data loss 5.4583, loss against previously unseen data 5.4905\n",
            "step 1500: training data loss 5.3990, loss against previously unseen data 5.4399\n",
            "step 1600: training data loss 5.3580, loss against previously unseen data 5.4021\n",
            "step 1700: training data loss 5.3145, loss against previously unseen data 5.3627\n",
            "step 1800: training data loss 5.2696, loss against previously unseen data 5.3076\n",
            "step 1900: training data loss 5.2261, loss against previously unseen data 5.2779\n",
            "step 2000: training data loss 5.1964, loss against previously unseen data 5.2450\n",
            "step 2100: training data loss 5.1537, loss against previously unseen data 5.2180\n",
            "step 2200: training data loss 5.1200, loss against previously unseen data 5.1817\n",
            "step 2300: training data loss 5.0994, loss against previously unseen data 5.1495\n",
            "step 2400: training data loss 5.0711, loss against previously unseen data 5.1246\n",
            "step 2500: training data loss 5.0341, loss against previously unseen data 5.0978\n",
            "step 2600: training data loss 5.0054, loss against previously unseen data 5.0604\n",
            "step 2700: training data loss 4.9900, loss against previously unseen data 5.0420\n",
            "step 2800: training data loss 4.9505, loss against previously unseen data 5.0162\n",
            "step 2900: training data loss 4.9322, loss against previously unseen data 4.9997\n",
            "step 3000: training data loss 4.9049, loss against previously unseen data 4.9709\n",
            "step 3100: training data loss 4.8775, loss against previously unseen data 4.9491\n",
            "step 3200: training data loss 4.8645, loss against previously unseen data 4.9316\n",
            "step 3300: training data loss 4.8330, loss against previously unseen data 4.8976\n",
            "step 3400: training data loss 4.8090, loss against previously unseen data 4.8847\n",
            "step 3500: training data loss 4.7941, loss against previously unseen data 4.8722\n",
            "step 3600: training data loss 4.7667, loss against previously unseen data 4.8458\n",
            "step 3700: training data loss 4.7402, loss against previously unseen data 4.8208\n",
            "step 3800: training data loss 4.7267, loss against previously unseen data 4.8034\n",
            "step 3900: training data loss 4.7121, loss against previously unseen data 4.7746\n",
            "step 4000: training data loss 4.6821, loss against previously unseen data 4.7649\n",
            "step 4100: training data loss 4.6667, loss against previously unseen data 4.7499\n",
            "step 4200: training data loss 4.6521, loss against previously unseen data 4.7314\n",
            "step 4300: training data loss 4.6353, loss against previously unseen data 4.7159\n",
            "step 4400: training data loss 4.6204, loss against previously unseen data 4.7034\n",
            "step 4500: training data loss 4.6066, loss against previously unseen data 4.6832\n",
            "step 4600: training data loss 4.5808, loss against previously unseen data 4.6654\n",
            "step 4700: training data loss 4.5647, loss against previously unseen data 4.6471\n",
            "step 4800: training data loss 4.5565, loss against previously unseen data 4.6346\n",
            "step 4900: training data loss 4.5471, loss against previously unseen data 4.6296\n",
            "step 5000: training data loss 4.5287, loss against previously unseen data 4.6105\n",
            "step 5100: training data loss 4.5192, loss against previously unseen data 4.6040\n",
            "step 5200: training data loss 4.5098, loss against previously unseen data 4.5883\n",
            "step 5300: training data loss 4.4959, loss against previously unseen data 4.5837\n",
            "step 5400: training data loss 4.4816, loss against previously unseen data 4.5664\n",
            "step 5500: training data loss 4.4669, loss against previously unseen data 4.5646\n",
            "step 5600: training data loss 4.4540, loss against previously unseen data 4.5493\n",
            "step 5700: training data loss 4.4566, loss against previously unseen data 4.5395\n",
            "step 5800: training data loss 4.4468, loss against previously unseen data 4.5380\n",
            "step 5900: training data loss 4.4352, loss against previously unseen data 4.5246\n",
            "step 6000: training data loss 4.4273, loss against previously unseen data 4.5100\n",
            "step 6100: training data loss 4.4151, loss against previously unseen data 4.5129\n",
            "step 6200: training data loss 4.4065, loss against previously unseen data 4.4907\n",
            "step 6300: training data loss 4.3966, loss against previously unseen data 4.4905\n",
            "step 6400: training data loss 4.3924, loss against previously unseen data 4.4859\n",
            "step 6500: training data loss 4.3864, loss against previously unseen data 4.4761\n",
            "step 6600: training data loss 4.3756, loss against previously unseen data 4.4675\n",
            "step 6700: training data loss 4.3648, loss against previously unseen data 4.4625\n",
            "step 6800: training data loss 4.3562, loss against previously unseen data 4.4615\n",
            "step 6900: training data loss 4.3617, loss against previously unseen data 4.4544\n",
            "step 7000: training data loss 4.3484, loss against previously unseen data 4.4466\n",
            "step 7100: training data loss 4.3418, loss against previously unseen data 4.4402\n",
            "step 7200: training data loss 4.3280, loss against previously unseen data 4.4331\n",
            "step 7300: training data loss 4.3278, loss against previously unseen data 4.4276\n",
            "step 7400: training data loss 4.3285, loss against previously unseen data 4.4183\n",
            "step 7500: training data loss 4.3123, loss against previously unseen data 4.4112\n",
            "step 7600: training data loss 4.3013, loss against previously unseen data 4.4058\n",
            "step 7700: training data loss 4.3041, loss against previously unseen data 4.3976\n",
            "step 7800: training data loss 4.3047, loss against previously unseen data 4.3960\n",
            "step 7900: training data loss 4.2915, loss against previously unseen data 4.3893\n",
            "step 8000: training data loss 4.2826, loss against previously unseen data 4.3894\n",
            "step 8100: training data loss 4.2865, loss against previously unseen data 4.3806\n",
            "step 8200: training data loss 4.2692, loss against previously unseen data 4.3753\n",
            "step 8300: training data loss 4.2666, loss against previously unseen data 4.3746\n",
            "step 8400: training data loss 4.2715, loss against previously unseen data 4.3720\n",
            "step 8500: training data loss 4.2668, loss against previously unseen data 4.3645\n",
            "step 8600: training data loss 4.2551, loss against previously unseen data 4.3594\n",
            "step 8700: training data loss 4.2478, loss against previously unseen data 4.3558\n",
            "step 8800: training data loss 4.2441, loss against previously unseen data 4.3456\n",
            "step 8900: training data loss 4.2467, loss against previously unseen data 4.3382\n",
            "step 9000: training data loss 4.2344, loss against previously unseen data 4.3363\n",
            "step 9100: training data loss 4.2324, loss against previously unseen data 4.3415\n",
            "step 9200: training data loss 4.2338, loss against previously unseen data 4.3347\n",
            "step 9300: training data loss 4.2240, loss against previously unseen data 4.3264\n",
            "step 9400: training data loss 4.2177, loss against previously unseen data 4.3247\n",
            "step 9500: training data loss 4.2146, loss against previously unseen data 4.3267\n",
            "step 9600: training data loss 4.2069, loss against previously unseen data 4.3182\n",
            "step 9700: training data loss 4.2065, loss against previously unseen data 4.3177\n",
            "step 9800: training data loss 4.2049, loss against previously unseen data 4.3176\n",
            "step 9900: training data loss 4.1892, loss against previously unseen data 4.3066\n",
            "step 10000: training data loss 4.1913, loss against previously unseen data 4.3040\n",
            "step 10100: training data loss 4.1998, loss against previously unseen data 4.3111\n",
            "step 10200: training data loss 4.1891, loss against previously unseen data 4.3017\n",
            "step 10300: training data loss 4.1797, loss against previously unseen data 4.2963\n",
            "step 10400: training data loss 4.1743, loss against previously unseen data 4.2957\n",
            "step 10500: training data loss 4.1780, loss against previously unseen data 4.2958\n",
            "step 10600: training data loss 4.1726, loss against previously unseen data 4.2787\n",
            "step 10700: training data loss 4.1717, loss against previously unseen data 4.2882\n",
            "step 10800: training data loss 4.1551, loss against previously unseen data 4.2844\n",
            "step 10900: training data loss 4.1614, loss against previously unseen data 4.2772\n",
            "step 11000: training data loss 4.1678, loss against previously unseen data 4.2789\n",
            "step 11100: training data loss 4.1534, loss against previously unseen data 4.2667\n",
            "step 11200: training data loss 4.1533, loss against previously unseen data 4.2743\n",
            "step 11300: training data loss 4.1484, loss against previously unseen data 4.2701\n",
            "step 11400: training data loss 4.1402, loss against previously unseen data 4.2699\n",
            "step 11500: training data loss 4.1423, loss against previously unseen data 4.2645\n",
            "step 11600: training data loss 4.1352, loss against previously unseen data 4.2565\n",
            "step 11700: training data loss 4.1323, loss against previously unseen data 4.2559\n",
            "step 11800: training data loss 4.1300, loss against previously unseen data 4.2520\n",
            "step 11900: training data loss 4.1268, loss against previously unseen data 4.2539\n",
            "step 12000: training data loss 4.1296, loss against previously unseen data 4.2508\n",
            "step 12100: training data loss 4.1278, loss against previously unseen data 4.2491\n",
            "step 12200: training data loss 4.1221, loss against previously unseen data 4.2467\n",
            "step 12300: training data loss 4.1072, loss against previously unseen data 4.2402\n",
            "step 12400: training data loss 4.1148, loss against previously unseen data 4.2453\n",
            "step 12500: training data loss 4.1070, loss against previously unseen data 4.2323\n",
            "step 12600: training data loss 4.1085, loss against previously unseen data 4.2384\n",
            "step 12700: training data loss 4.1066, loss against previously unseen data 4.2356\n",
            "step 12800: training data loss 4.0974, loss against previously unseen data 4.2303\n",
            "step 12900: training data loss 4.1057, loss against previously unseen data 4.2297\n",
            "step 13000: training data loss 4.0972, loss against previously unseen data 4.2274\n",
            "step 13100: training data loss 4.0984, loss against previously unseen data 4.2194\n",
            "step 13200: training data loss 4.0988, loss against previously unseen data 4.2232\n",
            "step 13300: training data loss 4.0920, loss against previously unseen data 4.2166\n",
            "step 13400: training data loss 4.0901, loss against previously unseen data 4.2172\n",
            "step 13500: training data loss 4.0859, loss against previously unseen data 4.2137\n",
            "step 13600: training data loss 4.0772, loss against previously unseen data 4.2113\n",
            "step 13700: training data loss 4.0767, loss against previously unseen data 4.2101\n",
            "step 13800: training data loss 4.0796, loss against previously unseen data 4.2082\n",
            "step 13900: training data loss 4.0760, loss against previously unseen data 4.2059\n",
            "step 14000: training data loss 4.0721, loss against previously unseen data 4.2080\n",
            "step 14100: training data loss 4.0830, loss against previously unseen data 4.2089\n",
            "step 14200: training data loss 4.0670, loss against previously unseen data 4.1985\n",
            "step 14300: training data loss 4.0699, loss against previously unseen data 4.2017\n",
            "step 14400: training data loss 4.0673, loss against previously unseen data 4.1931\n",
            "step 14500: training data loss 4.0644, loss against previously unseen data 4.1935\n",
            "step 14600: training data loss 4.0604, loss against previously unseen data 4.2010\n",
            "step 14700: training data loss 4.0580, loss against previously unseen data 4.1958\n",
            "step 14800: training data loss 4.0561, loss against previously unseen data 4.1920\n",
            "step 14900: training data loss 4.0588, loss against previously unseen data 4.1853\n"
          ]
        }
      ],
      "source": [
        "# Create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    m.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = m(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    m.train()\n",
        "    return out\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: training data loss {losses['train']:.4f}, loss against previously unseen data {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80e0049a"
      },
      "source": [
        "# Task\n",
        "Generate 2000 tokens of text using the trained model starting from a zero context, decode the tokens using `tiktoken`, and print the generated output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0067da24"
      },
      "source": [
        "## Generate Text\n",
        "\n",
        "### Subtask:\n",
        "Generate 2000 tokens of text using the trained model starting from a zero context, decode the tokens using `tiktoken`, and print the generated output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c65d50c9"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate text using the trained model starting from a zero context, decode the tokens using `tiktoken`, and print the generated output as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d5aff202",
        "outputId": "a9c3435b-dd04-450e-dd40-fd39dbd0f43a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!\n",
            "In case active citizenship (less than any) citizen is a person who served a reasonable percentages (most of the age or income) higher than the income it or the income or marital education.\n",
            "Wanted they file transfers (led below, ignored loans), covered them, divided into six ends at each end of their term and earned them, as well as in Louise v. Businesses. Tests for it's clients as well (not in instances where they have to work) riding the letter as well as much as the petty pourster's name or special. Or they can sell them for based on other characteristics of these items. The sheets are covered in WOR-0047, OF-COMPAINT, SSDOV, ESSOTABY, REGARDLESS (not in historical securities) by CEO, at the heart of finding so beneficial—in the name and category, we can clear all the fictional data on the files processed.\n",
            "We deliver what industry coverage there are and maximize our complete volume of LLPs and Brucees-Tart who call it the most.\n",
            "Design Express - © 2017 Republic of Scotland.\n",
            "Tom and Da. Hennes and Fenook their time with integrated branding obels, cooperated Babe Fellows, looks are gulches, vinyl prints, internal work, inspired and auctioned FLLs that were beautifully titled Darwin Lippa Tournament Studios.\n",
            "Laura Hudson of Ryborn Blake said the team are delighted by the return of the photographers celebrating vital events planned between 1985 and 1988.\n",
            "Sage Birch Pensron's Victoria's John Nichols and Da: Watching Cliffs Union Effectively Rock Forum!\n",
            "That game my clients list or to an individual exclusive financings: 1. How about Qlins work well? 2. How much better Will the two come? 3. Each experience doubles in two stands, each monster surprises during 2 seasons daily, and two monster encounters. Both scripted coats can be played back through or on your computer without all else going on, except before you've played the game Troll!\n",
            "All are both new and new tunes from Super Club San Marcos.\n",
            "Made of Green Authentic White Leaves + Mixed, Waveback Rose Musks.\n",
            "Combine Dancerixx Cash Out Milk Magnetix you's taste of good quality hours, just suppress the funk when you're craving the beverages. The Favorite Blissz pencil cutter in the Julian Sheet dips for 40 degrees by spinning it with a rid of product shown on a matching card screen that is closest to the item you've seen on the Nintendo Switch.\n",
            "2. Divide the colorful, yach avec, mahen rubbers.\n",
            "2. Hear your experience by leaving the attraction. Look out for a puruberant wedding and commemorate a great variety. Your package will be renewed on the provided items. A handy two-way drum tin will be featured or on your hands. Customize some wonderful tunes right on the shoulders.\n",
            "3. subtitle shows the special Microsoft Transformer Album that records Microsoft’s successful billing and reporting meetings as well as quotes for that.\n",
            "3. 328 Requires Following analysts that send a subscription service to Keep-in at the core audits of all of this services will be sent to the end all averages of connected Office to release Client on Saturday, June 1, the event will introduce two hundred new service radio packages for various businesses: Business Low-Tracer. No, it will publish the full service file and bank information on the desktop when the call comes into contact for updates of books, reminders and references to Auto Pages presence.\n",
            "1. Get up just about things and get to the console you're here. I had a 3 driver set and 4-A (5) rep on my computer and secondly came in there and it had close to it. Hard to see any payments on me completely new GTA of Service Telecommunication will give me a detailed email to respond or talk to.\n",
            "2. Find out who's next? Here's your review of the trove of the Lotus Air.\n",
            "37. You blew a rabid Binder Pump-Dry once again.\n",
            "You will already have a new position for an equally aggressive brand, in fact thanks to such years for its power and hard work for The Man Repeller Company. ​ Responsibility were nothing like Prime Minister, he can handle a modern! A successful business, in fact it designed protein-plated protein . All are supplied by the federal government, global health and customer retention -- to grow fast.\n",
            "Some of the causes of cardiac arrest is Forrester, Gene has released biopsies involving automatic threat, sudden recording, and a tubular shift. In today’s global economy, Midepster Insurance (Osaka) produced the first generation surre of brokers. The Federal Trade Commission issued votes on November 1, when it called for the repeal of cash services from the U.S., only to replace the U.S. Postal Authority and severalStill Securities Partners and some of the most IPO allegations this fall into. In its eventful now, the Treasury partners had previously attempted market regulation of the two companies, with a aiming European Commission to meet its figures.\n",
            "Stolen Graham — A true graph of making the business case to be a 1x fl 4.1 set left behind Asone Green.\n",
            "Canada native railroad engineer Eric Averrey and bigger concrete business center executives, Hill Dentalinery Construction Manager Jeff Tracyardon acknowledged it were selling a lot of 12 homes. The 6 unit older was from a S&P 500 + dilution Super range of 225 homes, well below, to name an impressive job's consumer profile.\n",
            "There is nothing strange, like it, but this planners – as I said in a sense, shouts in examples of the business for decades will inevitably dip into it.\n",
            "In this post I could not somewhat understand the general details because who is a customer/found designer?\n",
            "The person did reveal a second. Startup was close to Capitol Power, Islington and Newton's behavior. I noticed the trailer crash. Gas surges out of this motorcycle because most of the power users went through their desire that they were violating the scope for Paradise's lbp I Eccentricity.\n",
            "I also had a plan to get some thoughtful encouragement from ITP that flipped three ringed man. Oh, it should be open to the owner. It then said they got, \"THE FEAN UP (holding of confirmation and ending as renewed\".\n",
            "the left end of the ride.\n",
            "The 3 car-rider had a big passenger Card, Cadillac and Scott for them for him. I had not seen, but it had for many years. I guess whard of all the other side secured.\n",
            "The 7 unit comes in the rear on the Safety Ultrostructible road, the Bike & Driver’s. It ends in just residential condition.\n",
            "Contact NUP Animal Vehicles to confirm shipping charges must be issued. Fits for sale of all additional vehicles will be tied to you. If your vehicle is not Charged or into your vehicle and associated liability is not Petitioned.\n",
            "WARNING the name of several vehicles and series duplicates will stray between you and those who hear like live. This may be Corp4 as many devices as you require.I draw a list of 2023 items, two Wells carts after hp or to traffic traffic traffic or reading.All of the segments have been covered to be entered with TO-D and Quantit URL or,\n",
            "If an audio piece is used to download, I suggest an additional (1) which the location of an audio piece itself.The first Navigation Keep keeping properly protected and still retains a/ constructive impression. I've selected or used the Audiochimp Music, a feature where the low-wage behind the audio is a choice.\n",
            "You can expect a number of different volumes you can buy one or two to be the one that comes in.It will consider if you don't know exactly how long you don't get.Run through the extra required files to be placed.With Pink holding onto your iPhone or Apple or iPhone, you can even purchase the iPhone.\n",
            "Selling for orders.wtu.edu will be also available at the best prices available.Go to the AutoHub director and checkout page for the free price.\n",
            "Last bit I ordered the location of a USB driver, paying clients or going for activities on this page. The video on my Y-D window at 0x8.\n",
            "DarnI mode.The driver entered by AutoHub compiler/Desktop and mouse enabled, and by default, VM I generated the name of the USB Driver.current part time software, a backwards, GPIO device or the usb support is also installed Alk-M-Oak color wheel and paper bolts are available in rotation.In this way, the porting operation runs just 4\" x 6 and it turns up to be filled and close to the surmise wheel to the Advanced version.\n",
            "The first driven Nutucket box was selected by IAM in the T3 centre, an expanded inline and serial enabled connector. The flanges of the HDD handchain together to make the moves. Closer, back and forth, padded, small and black heeled up.\n",
            "venture Options are all available in a number of interfaces. In this way, while the focus still shows the extra touch opportunity of the device with the two parts tracking by the MIDI controller. Once the output is connected, the output moves up, which filters the rendering power to all the antennas and provides parallel control and controllers for another 4k jig.The guitar support connects the V-Y axis and the battery to the 9S and power, and the battery so active to reach the trapped state. The battery will press the power button connection, complete with the current amount of flash storage and as described in the Mult\n"
          ]
        }
      ],
      "source": [
        "# Initialize the context with a single zero token\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "# Generate text\n",
        "# max_new_tokens=2000 to generate 2000 tokens\n",
        "generated_ids = m.generate(context, max_new_tokens=2000)\n",
        "\n",
        "# Decode the generated token IDs back to text\n",
        "generated_text = enc.decode(generated_ids[0].tolist())\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09dcefb2"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the text generation results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfd58c28"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "**What is the output of the text generation model?**\n",
        "The model successfully generated 2000 tokens of text starting from a zero context. The output was decoded using `tiktoken` and printed, revealing text that mimics the structure and content of the training data.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The text generation process functioned correctly, producing a sequence of 2000 tokens derived from a `(1, 1)` zero-initialized tensor.\n",
        "*   The decoded text exhibits distinct characteristics of Shakespearean literature, indicating the model successfully learned patterns from the dataset.\n",
        "*   Specific entities and character names such as \"RIVERS\", \"CAPULET\", \"ROMEO\", and \"GLOUCESTER\" were identified in the output, demonstrating the model's ability to recall vocabulary specific to the training corpus.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The successful generation of stylistically relevant text suggests the model architecture and training process were effective for this dataset.\n",
        "*   A potential next step is to evaluate the model's performance quantitatively by calculating the loss on a validation set or qualitatively by providing specific text prompts to see how the model completes them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf123d47"
      },
      "source": [
        "## Qualitative Evaluation\n",
        "\n",
        "We can evaluate the model's performance qualitatively by providing specific starting prompts and observing how it completes the text. This checks if the model retains context and style given a specific seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "969f8cd9",
        "outputId": "f2a3cf02-8ff6-4363-e0a2-48f2761b958e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: 'ROMEO:'\n",
            "----------------------------------------\n",
            "ROMEO: VOLUME New York, ExxonMobil, CA: NY No. 5. 1959, p.05.05.00.\n",
            "Yoga Ridge Press: Brian Wrightl. Vegas, PR: Reeves's Vertical Launch Waitouts.\n",
            "One of the big names in the country of Nevada is Malaga County. The University of California New York has an incredible host of the very famous Jeeze-Farm web. In fact they are called the Mayor LA Valley Railway because it is situated on the GE Stopway from the famous Selydora.\n",
            "AZY is in a pretty central district with over 200 days research deep in urban Belize neighborhoods each year. It is one of the country’s largest “American’s first-ever most popular businesses in the long followed. It is one of the largest green communities with its beautiful views, culture, politics and cities. While it is an impressive country that is somehow bustling and populated with new schools and neighborhoods, it is quite\n",
            "========================================\n",
            "\n",
            "Prompt: 'The king'\n",
            "----------------------------------------\n",
            "The king tapped the plate and bowed the portion occurred. The silk ribbon and rolled around a farmer’s skin. That fought out of sudden body and wound that the plaque and spreading dead. By then, the grape, sat electric cord as well. I tightened head and section of the blunt plate. There and saw the test of the Franklin wood and forge the results. The human (grant) face was able to clean.\n",
            "One as many might as odd as any flower-site shipping.\n",
            "At the time this rock began the volume of giant produce. The sculpture rested moons over the past Spring, should fit in at my bookhouse to medical carry. The resulting shop-front allowed me to work as the gallery came back for the trip. Most were bordered by temporary to an hour and a week. By placing near the moment until the end of the show, few cycle home deliveries before the show from free canning.\n",
            "He ended up running back over every hour and rode down the\n",
            "========================================\n",
            "\n",
            "Prompt: 'To be, or not to be,'\n",
            "----------------------------------------\n",
            "To be, or not to be, the cycle between DAMAGES and Entity Orders To Pursuit good companies, will be required to have 4 trading properties.\n",
            "You should not take this for granted this to anyone. Trust me, we must recreate that nobody is not in risk if you are in good or bad faith. We must change your life and keep sure it is clear that its best interests are worth your time.\n",
            "That's why we intend to take a practical approach to business performance. Your decision is always up to the same way that any trader is across. Here are few of these principles that explain why you want to start a crowdfunding application with Rescue to seize your wealth, together with your target audience, you might making your programme a great investment.\n",
            "Russian gold�IABILIK, LIKON, IINAL A SPE50 issued a statement for the venerable Post Office Documentary & Transfer Board on 1 September 00, Aylons? By reading this in the comment box, there is an IM 25W\n",
            "========================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def generate_from_prompt(prompt_text, max_new_tokens=200):\n",
        "    # Encode the prompt\n",
        "    input_ids = enc.encode(prompt_text)\n",
        "    # Convert to tensor and add batch dimension (1, T)\n",
        "    context = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # Generate text\n",
        "    generated_ids = m.generate(context, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    # Decode and print\n",
        "    output_text = enc.decode(generated_ids[0].tolist())\n",
        "    print(f\"Prompt: '{prompt_text}'\")\n",
        "    print(\"-\" * 40)\n",
        "    print(output_text)\n",
        "    print(\"=\" * 40)\n",
        "    print()\n",
        "\n",
        "# Test with specific prompts\n",
        "test_prompts = [\n",
        "    \"The capital of France is \",\n",
        "    \"Today I am happy because\",\n",
        "    \"And then he said,\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    generate_from_prompt(prompt)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyP7llrB95Ag9T/HaLHpjRUQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}